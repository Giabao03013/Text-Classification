{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PreProcessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOfoYdoAXYVRCSyOOvG7ZYq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"6S7Xc4M4SyhL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624463575145,"user_tz":-420,"elapsed":22379,"user":{"displayName":"Truong Hoang Gia Bao B1609809","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtou-65Dwfppnq4KUy6WaS_PsZVMY9kT-QUuJC3g=s64","userId":"16980589557729516553"}},"outputId":"0ed8a535-cb46-4520-fc83-221da0988033"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D-F90AsCTtN3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624463745221,"user_tz":-420,"elapsed":164671,"user":{"displayName":"Truong Hoang Gia Bao B1609809","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtou-65Dwfppnq4KUy6WaS_PsZVMY9kT-QUuJC3g=s64","userId":"16980589557729516553"}},"outputId":"8b129f5f-7fc9-4235-db3b-405a464dffdc"},"source":["!pip install pyvi\n","!pip install underthesea\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pyvi\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/e1/0e5bc6b5e3327b9385d6e0f1b0a7c0404f28b74eb6db59a778515b30fd9c/pyvi-0.1-py2.py3-none-any.whl (8.5MB)\n","\u001b[K     |████████████████████████████████| 8.5MB 16.3MB/s \n","\u001b[?25hCollecting sklearn-crfsuite\n","  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n","Collecting python-crfsuite>=0.8.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n","\u001b[K     |████████████████████████████████| 747kB 33.6MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.0.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n","Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n","Successfully installed python-crfsuite-0.9.7 pyvi-0.1 sklearn-crfsuite-0.3.6\n","Collecting underthesea\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/5f/03ab9091b88e7851aa92da33f8eea6f111423cc1194cf1636c63c1fff3d0/underthesea-1.3.1-py3-none-any.whl (7.5MB)\n","\u001b[K     |████████████████████████████████| 7.5MB 2.7MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n","Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.9.7)\n","Collecting seqeval\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n","\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n","Collecting torch<=1.5.1,>=1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/cf/007b6de316c9f3d4cb315a60c308342cc299e464167f5ebc369e93b5e23a/torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2MB)\n","\u001b[K     |████████████████████████████████| 753.2MB 22kB/s \n","\u001b[?25hCollecting transformers<=3.5.1,>=3.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 40.6MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n","Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n","\u001b[K     |████████████████████████████████| 245kB 43.8MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.22.2.post1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.41.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval->underthesea) (1.19.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<=1.5.1,>=1.1.0->underthesea) (0.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 41.4MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 36.9MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.12.4)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 12.2MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2021.5.30)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<=3.5.1,>=3.5.0->underthesea) (2.4.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers<=3.5.1,>=3.5.0->underthesea) (57.0.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16184 sha256=f1c0d63e0b5839790b7802bd66633b99b02cab0767ec6c929df5cd2638844c95\n","  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n","Successfully built seqeval\n","\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n","Installing collected packages: seqeval, torch, sacremoses, sentencepiece, tokenizers, transformers, unidecode, underthesea\n","  Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","Successfully installed sacremoses-0.0.45 sentencepiece-0.1.91 seqeval-1.2.2 tokenizers-0.9.3 torch-1.5.1 transformers-3.5.1 underthesea-1.3.1 unidecode-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3aNFjcFxT4aR","executionInfo":{"status":"ok","timestamp":1624463867374,"user_tz":-420,"elapsed":4608,"user":{"displayName":"Truong Hoang Gia Bao B1609809","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtou-65Dwfppnq4KUy6WaS_PsZVMY9kT-QUuJC3g=s64","userId":"16980589557729516553"}}},"source":["import re\n","from os import listdir,sep\n","from underthesea import sent_tokenize\n","from pyvi import ViTokenizer"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MhdRR10UobQ","executionInfo":{"status":"ok","timestamp":1624463868439,"user_tz":-420,"elapsed":8,"user":{"displayName":"Truong Hoang Gia Bao B1609809","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtou-65Dwfppnq4KUy6WaS_PsZVMY9kT-QUuJC3g=s64","userId":"16980589557729516553"}}},"source":["def preProcess(text):\n","    text = re.sub(r'(?:(?:http|https):\\/\\/)?([-a-zA-Z0-9.]{2,256}\\.[a-z]{2,4})\\b(?:\\/[-a-zA-Z0-9@:%_\\+.~#?&//=]*)?',\n","                     \"\", text)\n","    text = re.sub(r'<[^>]+>', '', text)\n","    text = re.sub(r'[|\\^&+\\-%*\"/\"=!>]','',text)\n","    text = re.sub(r'[,?()\"\"]','',text)\n","    #text = re.sub(r'([^\\s\\w]|_)+', '', text)\n","    text = re.sub(r'\\d+','',text)\n","    #text = text.replace('.','. ')\n","    text = text.lower().strip()\n","    return text"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CvEXyK9UrXE","executionInfo":{"status":"ok","timestamp":1624468639798,"user_tz":-420,"elapsed":136388,"user":{"displayName":"Truong Hoang Gia Bao B1609809","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghtou-65Dwfppnq4KUy6WaS_PsZVMY9kT-QUuJC3g=s64","userId":"16980589557729516553"}}},"source":["from tqdm import tqdm\n","\n","\n","def loadData(data_folder):\n","    label_sent= ' '\n","    for folder in listdir(data_folder):\n","        #print(\"Load cat: \", folder)\n","        for file in listdir(data_folder + sep + folder):\n","            #print(\"Load cat: \", file)\n","            with open(data_folder + sep + folder + sep + file, 'r', encoding=\"utf-8\") as f:\n","                text = f.read()\n","                text = preProcess(text)\n","                sentences = sent_tokenize(text)\n","                label_sent += folder + ' ' + str(sentences[0:3]) +'\\n'\n","                s = open('gdrive/MyDrive/Text Classification/Data_full/Data_train_full.txt', 'w+', encoding='utf-8')\n","                s.write(label_sent)\n","def word_tokenize(file):\n","    r = open(file,'r',encoding='utf-8')\n","    sent = ''\n","    sentences_list=r.readlines()\n","    for sentece in sentences_list:\n","        tokenize = ViTokenizer.tokenize(sentece)\n","        sent += tokenize + '\\n'\n","    s = open('gdrive/MyDrive/Text Classification/Data_full/Train.txt','w+',encoding='utf-8')\n","    s.write(sent)\n","\n","loadData(\"gdrive/MyDrive/Text Classification/Data/Train\")\n","word_tokenize(\"gdrive/MyDrive/Text Classification/Data_full/Data_train_full.txt\")\n","\n"],"execution_count":15,"outputs":[]}]}